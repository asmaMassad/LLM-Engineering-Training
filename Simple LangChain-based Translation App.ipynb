{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca0020-e550-4402-a8cd-d529b0cd78dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U langchain langchain-mistralai FastAPI langserve langgraph sse_starlette nest-asyncio pyngrok uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471bdd4e-d91e-4008-80ac-87d7555ad4e6",
   "metadata": {},
   "source": [
    "# First LangChain-based Project - An LLM application that accepts a message and translates it into a target language\n",
    "The project is from the book \"Generative AI with Python and PyTorch: Navigating the AI frontier with LLMs, Stable Diffusion, and next-gen AI applications 2nd ed. Edition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57970ba3-58de-474e-a584-e90f051ab99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]= \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]= \"***\"\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"***\"\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd56e45e-6df2-419c-9465-11aaf2d03eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'messages = [\\n    SystemMessage(content=\"Translate the following from English into Arabic\"),\\n    HumanMessage(content=\"hi!\"),\\n]\\n\\nmodel.invoke(messages)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "'''messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Arabic\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3adb193-e2e8-41b3-a841-ae3c551b936b",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------\n",
    "This one of the examples used in the LangChain documentation, and we’ll build on this example by deploying the application we build using ngrok so that we can access a web application running in Collab asynchronously with the notebook process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf5fe3-a175-4e5f-9aed-cd94218e2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngrok config add-authtoken ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6532c786-5ce2-45b8-a234-9d3884167b1d",
   "metadata": {},
   "source": [
    "## Creating an LLM chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29267200-ff6b-491a-8a76-41e382aae7a8",
   "metadata": {},
   "source": [
    "We’ll need a few key ingredients here:\n",
    "1. The <span style=\"color: magenta;\">FastAPI framework</span> for web applications.\n",
    "2. The <span style=\"color: magenta;\">Uvicorn server</span> we’ll use to deploy the application once we’ve developed it.\n",
    "3. We’ll expose the model to end users on a particular URL using the <span style=\"color: magenta;\">add_routes function</span>.\n",
    "4. We’ll also need the <span style=\"color: magenta;\">PromptTemplate</span>, which allows us to specify how the model reads user input and what variables are expected.\n",
    "5. The <span style=\"color: magenta;\">StrOutputParser</span>, which converts “message” objects from LLMs to strings.\n",
    "6. We’ll use the <span style=\"color: magenta;\">nest_asynchio module</span> to run our FastAPI application inside the process thread of the Colab notebook.\n",
    "7. We’ll also need the<span style=\"color: magenta;\"> “chain” module</span> to connect different LangChain functions together in a series\n",
    "of sequential steps.\n",
    "8. Finally, we’ll also import the <span style=\"color: magenta;\">Mistral model API</span> for this example but, in theory,\n",
    "we could use any LLM in the LangChain library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29bf3b83-73e9-40ff-8692-e2dc9ba8b185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from pyngrok import ngrok\\n\\nngrok_tunnel = ngrok.connect(8000)\\nprint('Public URL:', ngrok_tunnel.public_url)\\nprint(ngrok_tunnel.public_url+'/chain/playground')\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from pyngrok import ngrok\n",
    "\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print('Public URL:', ngrok_tunnel.public_url)\n",
    "print(ngrok_tunnel.public_url+'/chain/playground')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "591cfdd4-5168-4ea5-97d2-e95b927a7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langserve import add_routes\n",
    "import nest_asyncio\n",
    "from langchain_core.runnables import chain\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940f2ec9-1ea3-49bb-8635-df93bc9e5f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "('system', system_template),\n",
    "('user', '{text}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d287f63a-a2d1-42c0-916b-a75ae027212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatMistralAI(model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08926f58-7ce1-45aa-b3bb-fb1c8b541dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af477fc-5dac-4ae9-9c04-cc48c21c58e8",
   "metadata": {},
   "source": [
    "Now, we can create our chain. In the LangChain library, individual operations may be created in a <span style=\"color: magenta;\">pipeline</span> or <span style=\"color: magenta;\">chain</span> using the <span style=\"color: magenta;\">pipe operator (|)</span>. The functions are executed from left to right, and the sequence can be saved to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfcf644a-a04b-44f4-81e2-a892b4437750",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb7d2ee-bedf-453d-98c8-94d0d69de718",
   "metadata": {},
   "source": [
    "### Creating the LLM application\n",
    "If we want to host our chain in the cloud, we can create a simple FastAPI server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98ea36be-c898-426f-9b3b-66166e69b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(\n",
    "title=\"LangChain Server\",\n",
    "version=\"1.0\",\n",
    "description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27b68dcc-9b2f-4f34-8e06-e31db76cbc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_routes(\n",
    "app,\n",
    "chain,\n",
    "path=\"/chain\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432116de-ea48-480e-a136-39168579bf95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c4db52e-b4fd-4fc1-ad80-7842b41e09c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: https://1695f666c44b.ngrok-free.app/chain/playground\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [8]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n",
      "    |  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n",
      "    |  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n",
      "    |  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n",
      "    |  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n",
      "    |_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n",
      "    \n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/chain/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  │\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  └──> /chain/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m See all available routes at /docs/\n",
      "INFO:     188.161.36.190:0 - \"GET /chain/playground HTTP/1.1\" 307 Temporary Redirect\n",
      "INFO:     188.161.36.190:0 - \"GET /chain/playground/ HTTP/1.1\" 200 OK\n",
      "INFO:     188.161.36.190:0 - \"GET /chain/playground/assets/index-400979f0.js HTTP/1.1\" 200 OK\n",
      "INFO:     188.161.36.190:0 - \"GET /chain/playground/assets/index-52e8ab2f.css HTTP/1.1\" 200 OK\n",
      "INFO:     188.161.36.190:0 - \"GET /chain/playground/favicon.ico HTTP/1.1\" 200 OK\n",
      "INFO:     188.161.36.190:0 - \"POST /chain/stream_log HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <async_generator object HTTP11ConnectionByteStream.__aiter__ at 0x000001BE4C674240>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LTC\\anaconda3\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 404, in __aiter__\n",
      "    yield part\n",
      "RuntimeError: async generator ignored GeneratorExit\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Original code:\n",
    "from pyngrok import ngrok\n",
    "\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print('Public URL:', ngrok_tunnel.public_url)\n",
    "print(ngrok_tunnel.public_url+'/chain/playground')\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import uvicorn\n",
    "\n",
    "# Start FastAPI app directly in the notebook\n",
    "uvicorn.run(app, host='0.0.0.0', port=8000)'''\n",
    "\n",
    "# New code to run server in background thread\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Start ngrok tunnel first\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print(\"Public URL:\", ngrok_tunnel.public_url + \"/chain/playground\")\n",
    "\n",
    "# Start FastAPI in background\n",
    "def run_server():\n",
    "    uvicorn.run(app, host='0.0.0.0', port=8000)\n",
    "\n",
    "thread = threading.Thread(target=run_server, daemon=True)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7184e95a-ae31-4df9-aa53-8e5ca2b0a432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19503886-5db0-47a7-9aed-7ac59fd1149a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
