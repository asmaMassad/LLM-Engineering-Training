{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5967d241-dd30-489b-a859-b142ee5b9b80",
   "metadata": {},
   "source": [
    "# Naive, Advanced, and Modular RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6394203a-61be-497e-989e-663fd65910ce",
   "metadata": {},
   "source": [
    "<span style=\"color: aqua; font-size:120%\">The examples are from the book \"RAG-Driven-Generative-AI\", but here I am using Mistral AI instead of openAI</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f5257-0177-48b0-894d-93a4c8e6058d",
   "metadata": {},
   "source": [
    "## Foundations and Basic Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c8228b-77c9-450a-bdb2-ca2a46d8acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ff9387-79c9-4141-9c60-5d566fa1ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === CONFIGURATION ===\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\", \"***\")\n",
    "MISTRAL_API_URL = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "MODEL_NAME = \"mistral-large-latest\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33d5801-33fe-4a12-b9b4-a67a71049668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_mistral_with_full_text(itext):\n",
    "    # Join all lines into a single prompt\n",
    "    text_input = '\\n'.join(itext)\n",
    "    prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {MISTRAL_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "         \"messages\":[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "         ],\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(MISTRAL_API_URL, json=payload, headers=headers)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "129bdf74-da4c-419d-944a-bd807cf1f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_records = [\n",
    "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
    "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
    "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
    "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
    "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
    "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
    "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
    "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
    "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
    "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
    "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
    "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
    "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
    "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
    "    \"The retrieved documents are then fed into the language model.\",\n",
    "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
    "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
    "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
    "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
    "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
    "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
    "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
    "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
    "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
    "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
    "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
    "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
    "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "189312dd-30d9-4344-8fe1-ba837ab100fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
      "in the field of artificial intelligence, particularly within the realm of\n",
      "natural language processing (NLP). It innovatively combines the capabilities of\n",
      "neural network-based language models with retrieval systems to enhance the\n",
      "generation of text, making it more accurate, informative, and contextually\n",
      "relevant. This methodology leverages the strengths of both generative and\n",
      "retrieval architectures to tackle complex tasks that require not only linguistic\n",
      "fluency but also factual correctness and depth of knowledge. At the core of\n",
      "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
      "transformer-based neural network, similar to those used in models like GPT\n",
      "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
      "Representations from Transformers). This component is responsible for producing\n",
      "coherent and contextually appropriate language outputs based on a mixture of\n",
      "input prompts and additional information fetched by the retrieval component.\n",
      "Complementing the language model is the retrieval system, which is usually built\n",
      "on a database of documents or a corpus of texts. This system uses techniques\n",
      "from information retrieval to find and fetch documents that are relevant to the\n",
      "input query or prompt. The mechanism of relevance determination can range from\n",
      "simple keyword matching to more complex semantic search algorithms which\n",
      "interpret the meaning behind the query to find the best matches. This component\n",
      "merges the outputs from the language model and the retrieval system. It\n",
      "effectively synthesizes the raw data fetched by the retrieval system into the\n",
      "generative process of the language model. The integrator ensures that the\n",
      "information from the retrieval system is seamlessly incorporated into the final\n",
      "text output, enhancing the model's ability to generate responses that are not\n",
      "only fluent and grammatically correct but also rich in factual details and\n",
      "context-specific nuances. When a query or prompt is received, the system first\n",
      "processes it to understand the requirement or the context. Based on the\n",
      "processed query, the retrieval system searches through its database to find\n",
      "relevant documents or information snippets. This retrieval is guided by the\n",
      "similarity of content in the documents to the query, which can be determined\n",
      "through various techniques like vector embeddings or semantic similarity\n",
      "measures. The retrieved documents are then fed into the language model. In some\n",
      "implementations, this integration happens at the token level, where the model\n",
      "can access and incorporate specific pieces of information from the retrieved\n",
      "texts dynamically as it generates each part of the response. The language model,\n",
      "now augmented with direct access to retrieved information, generates a response.\n",
      "This response is not only influenced by the training of the model but also by\n",
      "the specific facts and details contained in the retrieved documents, making it\n",
      "more tailored and accurate. By directly incorporating information from external\n",
      "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
      "are more factual and relevant to the given query. This is particularly useful in\n",
      "domains like medical advice, technical support, and other areas where precision\n",
      "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
      "systems can dynamically adapt to new information since they retrieve data in\n",
      "real-time from their databases. This allows them to remain current with the\n",
      "latest knowledge and trends without needing frequent retraining. With access to\n",
      "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
      "provide detailed and nuanced answers that a standalone language model might not\n",
      "be capable of generating based solely on its pre-trained knowledge. While\n",
      "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
      "with its challenges. These include the complexity of integrating retrieval and\n",
      "generation systems, the computational overhead associated with real-time data\n",
      "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
      "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
      "of the retrieved information remains a significant challenge, as does managing\n",
      "the potential for introducing biases or errors from the external sources. In\n",
      "summary, Retrieval Augmented Generation represents a significant advancement in\n",
      "the field of artificial intelligence, merging the best of retrieval-based and\n",
      "generative technologies to create systems that not only understand and generate\n",
      "natural language but also deeply comprehend and utilize the vast amounts of\n",
      "information available in textual form. A RAG vector store is a database or\n",
      "dataset that contains vectorized data points.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "paragraph = ' '.join(db_records)\n",
    "wrapped_text = textwrap.fill(paragraph, width=80)\n",
    "print(wrapped_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbab2a0f-0e74-4362-9b97-8fc737655bca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mistral AI Response:\n",
      " A **RAG (Retrieval-Augmented Generation) Store** is a specialized data storage and retrieval system designed to support **Retrieval-Augmented Generation (RAG)**, a hybrid AI framework that combines **information retrieval** with **generative language models** (like LLMs). The RAG store acts as the backbone for fetching relevant contextual data to enhance the accuracy, relevance, and factual grounding of AI-generated responses.\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Components of a RAG Store**\n",
      "A RAG store typically consists of the following elements:\n",
      "\n",
      "#### 1. **Data Ingestion Layer**\n",
      "   - **Purpose**: Collects and preprocesses raw data (e.g., documents, databases, APIs, or web content) to make it searchable.\n",
      "   - **Processes**:\n",
      "     - **Data Cleaning**: Removes noise (e.g., HTML tags, duplicates, or irrelevant metadata).\n",
      "     - **Chunking**: Splits large documents into smaller, manageable segments (e.g., paragraphs or sentences) to enable granular retrieval.\n",
      "     - **Metadata Tagging**: Adds labels (e.g., source, author, date) to improve contextual filtering.\n",
      "   - **Example Tools**: Apache Tika (for text extraction), LangChain (for chunking), or custom scripts.\n",
      "\n",
      "#### 2. **Embedding Model**\n",
      "   - **Purpose**: Converts text chunks into **vector embeddings** (numerical representations) to enable semantic search.\n",
      "   - **How it Works**:\n",
      "     - Uses models like **Sentence-BERT**, **OpenAI’s `text-embedding-ada-002`**, or **Hugging Face’s `all-MiniLM-L6-v2`** to map text to high-dimensional vectors.\n",
      "     - Embeddings capture semantic meaning, so similar content has similar vectors.\n",
      "   - **Example**:\n",
      "     - Input: *\"What are the symptoms of diabetes?\"*\n",
      "     - Output: A 384-dimensional vector representing the query’s meaning.\n",
      "\n",
      "#### 3. **Vector Database (or Index)**\n",
      "   - **Purpose**: Stores embeddings and enables fast, similarity-based retrieval.\n",
      "   - **Key Features**:\n",
      "     - **Approximate Nearest Neighbor (ANN) Search**: Efficiently finds the most relevant chunks without exhaustive search (e.g., using **FAISS**, **Pinecone**, or **Weaviate**).\n",
      "     - **Scalability**: Handles millions of embeddings with low latency.\n",
      "     - **Hybrid Search**: Combines vector search with keyword-based filters (e.g., metadata like \"source=NIH\").\n",
      "   - **Example Databases**:\n",
      "     - **Open-source**: FAISS (Facebook), Milvus, Qdrant.\n",
      "     - **Managed**: Pinecone, Weaviate, Supabase (with pgvector).\n",
      "\n",
      "#### 4. **Retrieval Layer**\n",
      "   - **Purpose**: Fetches the top-*k* most relevant chunks for a given query.\n",
      "   - **Methods**:\n",
      "     - **Semantic Search**: Ranks chunks by cosine similarity between query and document embeddings.\n",
      "     - **Hybrid Search**: Combines semantic + keyword/BM25 (e.g., using **Elasticsearch** or **Typesense**).\n",
      "     - **Reranking**: Uses cross-encoders (e.g., **ColBERT**) to refine results post-retrieval.\n",
      "   - **Example**:\n",
      "     - Query: *\"How does photosynthesis work?\"*\n",
      "     - Retrieved Chunks: Top 3 paragraphs from biology textbooks with high semantic similarity.\n",
      "\n",
      "#### 5. **Augmentation Layer**\n",
      "   - **Purpose**: Prepares retrieved context for the generative model.\n",
      "   - **Processes**:\n",
      "     - **Context Truncation**: Limits context to the model’s token limit (e.g., 4,096 tokens for `gpt-3.5-turbo`).\n",
      "     - **Prompt Engineering**: Formats the context + query into a prompt template (e.g., *\"Answer using only this context: [retrieved chunks]\"*).\n",
      "     - **Citation Handling**: Adds references (e.g., *\"Source: [Document X]\"*).\n",
      "\n",
      "#### 6. **Generative Model (LLM)**\n",
      "   - **Purpose**: Generates a response using the retrieved context.\n",
      "   - **How it Works**:\n",
      "     - The LLM (e.g., **GPT-4**, **Llama 2**, or **Mistral**) takes the augmented prompt and produces an answer grounded in the retrieved data.\n",
      "     - Techniques like **few-shot learning** or **chain-of-thought** can be applied.\n",
      "   - **Example Output**:\n",
      "     > *\"Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen. It occurs in the chloroplasts and involves two stages: light-dependent reactions and the Calvin cycle (Source: Biology Textbook, 2020).\"*\n",
      "\n",
      "#### 7. **Feedback Loop (Optional)**\n",
      "   - **Purpose**: Improves the RAG system over time.\n",
      "   - **Methods**:\n",
      "     - **User Feedback**: Logs corrections (e.g., \"This answer is outdated\") to update the data store.\n",
      "     - **Reinforcement Learning**: Fine-tunes retrieval or generation based on user interactions.\n",
      "     - **Evaluation Metrics**: Tracks **precision@k**, **recall**, or **answer faithfulness** (e.g., using **Ragas** or **TruLens**).\n",
      "\n",
      "---\n",
      "\n",
      "### **Types of RAG Stores**\n",
      "1. **Static RAG Store**\n",
      "   - **Use Case**: Fixed knowledge bases (e.g., company wikis, legal documents).\n",
      "   - **Example**: A FAQ bot for a product manual where data rarely changes.\n",
      "\n",
      "2. **Dynamic RAG Store**\n",
      "   - **Use Case**: Real-time or frequently updated data (e.g., news, social media).\n",
      "   - **Example**: A chatbot that fetches the latest stock market trends from APIs.\n",
      "\n",
      "3. **Hybrid RAG Store**\n",
      "   - **Use Case**: Combines structured (SQL) and unstructured (text) data.\n",
      "   - **Example**: A healthcare assistant retrieving patient records (SQL) + medical research papers (PDFs).\n",
      "\n",
      "4. **Multi-Modal RAG Store**\n",
      "   - **Use Case**: Retrieves images, tables, or videos alongside text.\n",
      "   - **Example**: A system that answers questions about diagrams in a textbook.\n",
      "\n",
      "---\n",
      "\n",
      "### **How a RAG Store Works: Step-by-Step**\n",
      "1. **Indexing Phase**:\n",
      "   - Raw data → Chunking → Embedding → Stored in vector DB.\n",
      "2. **Query Phase**:\n",
      "   - User asks: *\"What causes climate change?\"*\n",
      "   - Query is embedded → Vector DB retrieves top 3 relevant chunks.\n",
      "   - Chunks are formatted into a prompt: *\"Use this context: [Chunk 1], [Chunk 2], [Chunk 3]. Answer: What causes climate change?\"*\n",
      "   - LLM generates a response using the context.\n",
      "3. **Response**:\n",
      "   > *\"Climate change is primarily caused by human activities like burning fossil fuels (coal, oil), which release greenhouse gases (CO₂, methane) into the atmosphere. Deforestation and industrial processes also contribute (Source: IPCC Report, 2021).\"*\n",
      "\n",
      "---\n",
      "\n",
      "### **Advantages of a RAG Store**\n",
      "1. **Factual Accuracy**: Reduces hallucinations by grounding responses in retrieved data.\n",
      "2. **Customizability**: Can be tailored to specific domains (e.g., legal, medical).\n",
      "3. **Cost-Effective**: Avoids fine-tuning large models; updates data without retraining.\n",
      "4. **Transparency**: Provides citations/sources for answers.\n",
      "5. **Scalability**: Handles large datasets efficiently with vector DBs.\n",
      "\n",
      "---\n",
      "\n",
      "### **Challenges and Solutions**\n",
      "| **Challenge**               | **Solution**                                  |\n",
      "|-----------------------------|-----------------------------------------------|\n",
      "| **Retrieval Noise**         | Use rerankers (e.g., ColBERT) or hybrid search. |\n",
      "| **Outdated Data**           | Implement real-time syncs or user feedback loops. |\n",
      "| **Context Overload**        | Truncate or summarize retrieved chunks.       |\n",
      "| **Latency**                 | Optimize vector DB (e.g., quantization in FAISS). |\n",
      "| **Bias in Data**            | Curate diverse sources and audit retrievals.  |\n",
      "\n",
      "---\n",
      "\n",
      "### **Tools to Build a RAG Store**\n",
      "| **Component**       | **Tools/Libraries**                          |\n",
      "|---------------------|---------------------------------------------|\n",
      "| **Data Ingestion**  | LangChain, Unstructured.io, Apache Tika     |\n",
      "| **Embedding Models**| Sentence-BERT, OpenAI Embeddings, Hugging Face |\n",
      "| **Vector DBs**      | Pinecone, Weaviate, FAISS, Milvus, Qdrant    |\n",
      "| **Retrieval**       | Elasticsearch, Typesense, Vespa             |\n",
      "| **LLMs**            | OpenAI GPT-4, Llama 2, Mistral, Claude      |\n",
      "| **Evaluation**      | Ragas, TruLens, Arize                       |\n",
      "\n",
      "---\n",
      "\n",
      "### **Example Architecture**\n",
      "```plaintext\n",
      "User Query → [Embedding Model] → [Vector DB (Retrieve Top-k)] → [Prompt Augmentation] → [LLM] → Response\n",
      "                          ↑\n",
      "                     Feedback Loop\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Use Cases**\n",
      "1. **Customer Support**: Retrieve product manuals to answer user queries.\n",
      "2. **Legal/Compliance**: Fetch relevant case laws or regulations.\n",
      "3. **Healthcare**: Provide evidence-based medical advice from research papers.\n",
      "4. **Education**: Answer student questions using textbooks or lecture notes.\n",
      "5. **Enterprise Search**: Replace traditional keyword search with semantic understanding.\n",
      "\n",
      "---\n",
      "\n",
      "### **Comparison: RAG vs. Fine-Tuning**\n",
      "| **Feature**         | **RAG**                                      | **Fine-Tuning**                          |\n",
      "|---------------------|-----------------------------------------------|------------------------------------------|\n",
      "| **Data Updates**    | Easy (just update the store)                  | Requires retraining.                    |\n",
      "| **Cost**            | Lower (no GPU-heavy training)                 | Higher (GPU costs for training).        |\n",
      "| **Hallucinations**  | Reduced (grounded in data)                   | Possible if training data is noisy.     |\n",
      "| **Domain Adaptation** | Fast (swap data store)                      | Slow (retrain model).                   |\n",
      "| **Latency**         | Higher (retrieval + generation)               | Lower (single model inference).         |\n",
      "\n",
      "---\n",
      "\n",
      "### **Future Trends**\n",
      "1. **Agentic RAG**: Combines RAG with autonomous agents for multi-step reasoning (e.g., **AutoGPT**).\n",
      "2. **Small Specialized Stores**: Micro-RAGs for niche topics (e.g., a RAG store just for Python debugging).\n",
      "3. **Real-Time RAG**: Streaming updates (e.g., live sports stats or stock prices).\n",
      "4. **Multimodal RAG**: Retrieving images, audio, or video alongside text.\n",
      "5. **Self-Improving RAG**: Uses reinforcement learning to optimize retrieval/generation.\n",
      "\n",
      "---\n",
      "### **How to Build Your Own RAG Store**\n",
      "1. **Step 1**: Collect and preprocess your data (e.g., PDFs, CSVs).\n",
      "2. **Step 2**: Choose an embedding model (e.g., `all-MiniLM-L6-v2`).\n",
      "3. **Step 3**: Set up a vector DB (e.g., FAISS for local, Pinecone for cloud).\n",
      "4. **Step 4**: Implement retrieval logic (semantic + keyword).\n",
      "5. **Step 5**: Integrate with an LLM (e.g., OpenAI API or Llama 2).\n",
      "6. **Step 6**: Add evaluation (e.g., check answer faithfulness).\n",
      "7. **Step 7**: Deploy and monitor (e.g., track retrieval precision).\n",
      "\n",
      "---\n",
      "### **Example Code Snippet (Python)**\n",
      "```python\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.embeddings import HuggingFaceEmbeddings\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.chains import RetrievalQA\n",
      "\n",
      "# 1. Load data and split into chunks\n",
      "from langchain.document_loaders import TextLoader\n",
      "loader = TextLoader(\"data.txt\")\n",
      "documents = loader.load_and_split()\n",
      "\n",
      "# 2. Create embeddings and store in FAISS\n",
      "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "db = FAISS.from_documents(documents, embeddings)\n",
      "\n",
      "# 3. Set up RAG chain\n",
      "llm = OpenAI(temperature=0)\n",
      "qa_chain = RetrievalQA.from_chain_type(\n",
      "    llm=llm,\n",
      "    retriever=db.as_retriever(),\n",
      "    chain_type=\"stuff\"\n",
      ")\n",
      "\n",
      "# 4. Query the RAG store\n",
      "query = \"What is the capital of France?\"\n",
      "response = qa_chain.run(query)\n",
      "print(response)  # \"The capital of France is Paris (Source: data.txt).\"\n",
      "```\n",
      "\n",
      "---\n",
      "### **Key Takeaways**\n",
      "- A **RAG store** is the **data infrastructure** that powers Retrieval-Augmented Generation.\n",
      "- It **retrieves relevant context** to augment LLM responses, improving accuracy and transparency.\n",
      "- Components include **data ingestion, embeddings, vector DBs, retrieval logic, and LLMs**.\n",
      "- Use cases span **customer support, healthcare, legal, and education**.\n",
      "- Challenges like **latency, noise, and outdated data** can be mitigated with hybrid search, rerankers, and feedback loops.\n",
      "- Tools like **LangChain, FAISS, and Pinecone** simplify building RAG stores.\n",
      "\n",
      "Would you like a deeper dive into any specific aspect (e.g., evaluation metrics, advanced retrieval techniques)?\n"
     ]
    }
   ],
   "source": [
    "query = [\"define a rag store\"]\n",
    "\n",
    "llm_response = call_mistral_with_full_text(query)\n",
    "print(\"\\nMistral AI Response:\\n\", llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f98fee-8a02-4b5d-b033-875accbc03fc",
   "metadata": {},
   "source": [
    "## Advanced Techniques and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b6719-85f4-46a9-8293-9ed61bd32624",
   "metadata": {},
   "source": [
    "### 1. Retrieval Metrics\n",
    "\n",
    "Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c09583-5f8b-4e1c-b7f8-48eefcb9c551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LTC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c5d4cf-1a4c-4033-8092-6c4eeae78ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eef9713-9208-4149-88b7-70c6eae21a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f95d63e7-1677-4b5a-a3e7-5a8d1f3211c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        use_idf=True,\n",
    "        norm='l2',\n",
    "        ngram_range=(1, 2), # Use unigrams and bigrams\n",
    "        sublinear_tf=True, # Apply sublinear TF scaling\n",
    "        analyzer='word' # You could also experiment with 'c\n",
    "    )\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b2838-9d6e-4d12-941e-7eecd283dabb",
   "metadata": {},
   "source": [
    "Enhanced similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f1df182-12e8-493f-af24-419256e51e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        lemmatized_words.append(token.lemma_)\n",
    "    return lemmatized_words\n",
    "\n",
    "def expand_with_synonyms(words):\n",
    "    expanded_words = words.copy()\n",
    "    for word in words:\n",
    "        expanded_words.extend(get_synonyms(word))\n",
    "    return expanded_words\n",
    "\n",
    "def calculate_enhanced_similarity(text1, text2):\n",
    "    # Preprocess and tokenize texts\n",
    "    words1 = preprocess_text(text1)\n",
    "    words2 = preprocess_text(text2)\n",
    "\n",
    "    # Expand with synonyms\n",
    "    words1_expanded = expand_with_synonyms(words1)\n",
    "    words2_expanded = expand_with_synonyms(words2)\n",
    "\n",
    "    # Count word frequencies\n",
    "    freq1 = Counter(words1_expanded)\n",
    "    freq2 = Counter(words2_expanded)\n",
    "\n",
    "    # Create a set of all unique words\n",
    "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
    "\n",
    "    # Create frequency vectors\n",
    "    vector1 = [freq1[word] for word in unique_words]\n",
    "    vector2 = [freq2[word] for word in unique_words]\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849feca-81c0-4ea2-a64b-562d798d3f40",
   "metadata": {},
   "source": [
    "### 2. Naive RAG\n",
    "Keyword search and matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04938d52-db55-4f78-adfd-7a3e01350192",
   "metadata": {},
   "source": [
    "In this naïve RAG function, we will implement keyword search and matching. To achieve this, we will apply a straightforward retrieval method in the code:\n",
    "1. Split the query into individual keywords\n",
    "2. Split each record in the dataset into keywords\n",
    "3. Determine the length of the common matches\n",
    "4. Choose the record with the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ce37f51-785b-479d-bc96-373ea042f3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Keyword Score: 3\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_best_match_keyword_search(query, db_records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "\n",
    "    # Split the query into individual keywords\n",
    "    query_keywords = set(query.lower().split())\n",
    "\n",
    "    # Iterate through each record in db_records\n",
    "    for record in db_records:\n",
    "        # Split the record into keywords\n",
    "        record_keywords = set(record.lower().split())\n",
    "\n",
    "        # Calculate the number of common keywords\n",
    "        common_keywords = query_keywords.intersection(record_keywords)\n",
    "        current_score = len(common_keywords)\n",
    "\n",
    "        # Update the best score and record if the current score is higher\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "\n",
    "    return best_score, best_record\n",
    "\n",
    "def print_formatted_response(response):\n",
    "    # Define the width for wrapping the text\n",
    "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
    "    wrapped_text = wrapper.fill(text=response)\n",
    "\n",
    "    # Print the formatted response with a header and footer\n",
    "    print(\"Response:\")\n",
    "    print(\"---------------\")\n",
    "    print(wrapped_text)\n",
    "    print(\"---------------\\n\")\n",
    "query = \"define a rag store\"\n",
    "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
    "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
    "\n",
    "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be80c3-e160-4266-94b2-3747ebe310d5",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f20070e-8bf5-4f56-8b94-fc345fde3eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.126\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "score = calculate_cosine_similarity(query, best_matching_record)\n",
    "print(f\"Best Cosine Similarity Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5d7abb4-de8d-43c5-bfba-fb3ec3880d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197f5b5-7829-4fbf-84b1-c6e931a6b11e",
   "metadata": {},
   "source": [
    "### Augmented input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cca753a-a88c-4db1-9063-735124198ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_input=query+ \": \"+ best_matching_record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e14f7d68-1552-4380-9106-2f1c8765fd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef3f1ee-4a5b-4516-940b-d642217dd0a1",
   "metadata": {},
   "source": [
    "### Generation|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b699591e-4185-48cf-a889-6ba3aa66df8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "The term you’ve provided is **\"RAG vector store\"** (or **Retrieval-Augmented\n",
      "Generation vector store**), a key component in modern **Retrieval-Augmented\n",
      "Generation (RAG)** systems. Below is a detailed breakdown of what it means, how\n",
      "it works, and its significance in NLP and AI applications.  ---  ### **1.\n",
      "Definition: What is a RAG Vector Store?** A **RAG vector store** is a\n",
      "specialized **database or data structure** designed to store **vectorized data\n",
      "points** (embeddings) for efficient **retrieval and similarity search**. It is a\n",
      "core component of **Retrieval-Augmented Generation (RAG)**, a framework that\n",
      "combines: - **Retrieval** (fetching relevant information from a knowledge base)\n",
      "- **Generation** (using a language model to produce responses based on retrieved\n",
      "data).  The vector store holds **pre-computed embeddings** (numerical\n",
      "representations) of text, images, or other data, enabling fast lookup of\n",
      "semantically similar content.  ---  ### **2. Key Components of a RAG Vector\n",
      "Store** #### **A. Vector Embeddings** - Data (e.g., text documents, code\n",
      "snippets, or product descriptions) is converted into **dense vectors**\n",
      "(embeddings) using models like:   - **Sentence-BERT** (for text)   - **CLIP**\n",
      "(for images + text)   - **OpenAI’s `text-embedding-ada-002`** - Example:   - The\n",
      "sentence *\"The cat sat on the mat\"* → `[0.12, -0.45, 0.78, ..., 0.23]` (a\n",
      "384/768/1536-dimensional vector).  #### **B. Storage Backend** The vector store\n",
      "can be implemented using: - **Approximate Nearest Neighbor (ANN) databases**:\n",
      "- **FAISS** (Facebook AI Similarity Search)   - **Milvus** / **Zilliz**   -\n",
      "**Pinecone**   - **Weaviate**   - **Chroma**   - **Redis** (with vector search\n",
      "modules) - **Traditional databases with vector extensions**:   - **PostgreSQL\n",
      "(pgvector)**   - **Elasticsearch (with dense vector support)**  #### **C.\n",
      "Indexing for Fast Retrieval** - Vectors are indexed to enable **sub-millisecond\n",
      "similarity searches** (e.g., cosine similarity, L2 distance). - Techniques like\n",
      "**Locality-Sensitive Hashing (LSH)** or **Hierarchical Navigable Small World\n",
      "(HNSW)** optimize search speed.  ---  ### **3. How a RAG Vector Store Works**\n",
      "#### **Step 1: Data Ingestion & Embedding** - Raw data (e.g., Wikipedia\n",
      "articles, PDFs, or product catalogs) is chunked into smaller segments. - Each\n",
      "chunk is converted into a vector using an embedding model. - Vectors are stored\n",
      "in the vector store with metadata (e.g., source URL, document ID).  #### **Step\n",
      "2: Query Processing** - A user asks a question (e.g., *\"What causes climate\n",
      "change?\"*). - The question is embedded into a vector using the same model.  ####\n",
      "**Step 3: Retrieval** - The vector store performs a **similarity search** to\n",
      "find the top-*k* most relevant chunks (e.g., documents about greenhouse gases).\n",
      "- Techniques:   - **Exact search** (slow but precise, e.g., brute-force cosine\n",
      "similarity).   - **Approximate search** (faster, e.g., FAISS with IVF-PQ).  ####\n",
      "**Step 4: Augmented Generation** - The retrieved chunks are fed into a **large\n",
      "language model (LLM)** (e.g., Llama, GPT-4) as context. - The LLM generates a\n",
      "response grounded in the retrieved data (reducing hallucinations).  --- ### **4.\n",
      "Why Use a RAG Vector Store?** | **Problem**               | **Solution via RAG\n",
      "Vector Store**                          | |---------------------------|---------\n",
      "--------------------------------------------------| | **Hallucinations in LLMs**\n",
      "| Retrieves factual data to ground responses.               | | **Out-of-date\n",
      "knowledge**  | Dynamically fetches updated information (vs. static LLM training\n",
      "data). | | **Domain-specific QA**     | Stores private/corporate data (e.g.,\n",
      "internal wikis).     | | **Scalability**            | ANN databases handle\n",
      "millions of vectors efficiently.     | | **Cost efficiency**        | Cheaper\n",
      "than fine-tuning LLMs for every use case.          |  ---  ### **5. Example\n",
      "Workflow** **Use Case**: A chatbot answering questions about a company’s\n",
      "internal documents.  1. **Setup**:    - Documents (PDFs, Confluence pages) are\n",
      "split into chunks.    - Chunks are embedded using `sentence-transformers/all-\n",
      "mpnet-base-v2` and stored in **Weaviate**.  2. **Query**:    - User asks:\n",
      "*\"What’s our return policy for damaged items?\"*    - The question is embedded\n",
      "into a vector.  3. **Retrieval**:    - Weaviate returns the top 3 chunks\n",
      "matching the query vector (e.g., sections from the \"Returns FAQ\" document).  4.\n",
      "**Generation**:    - The LLM (e.g., Mistral-7B) uses the retrieved chunks to\n",
      "generate:      *\"According to our policy (v2.1, 2023), damaged items can be\n",
      "returned within 30 days with proof of purchase. Contact support@company.com for\n",
      "assistance.\"*  ---  ### **6. Popular RAG Vector Store Tools** | **Tool**       |\n",
      "**Type**               | **Key Features**                                  | |--\n",
      "--------------|------------------------|----------------------------------------\n",
      "-----------| | **FAISS**      | Library (Meta)         | GPU-optimized, supports\n",
      "IVF-PQ.                  | | **Pinecone**   | Managed SaaS           |\n",
      "Serverless, hybrid search (dense + sparse).      | | **Weaviate**   | Open-\n",
      "source DB         | Graph-based, supports cross-references.          | |\n",
      "**Chroma**     | Open-source            | Lightweight, easy to deploy locally.\n",
      "| | **Milvus**     | Open-source (Zilliz)   | Scalable, supports billions of\n",
      "vectors.          | | **pgvector**   | PostgreSQL extension   | SQL + vector\n",
      "search in one database.             |  ---  ### **7. Challenges &\n",
      "Considerations** - **Embedding Quality**: Poor embeddings → poor retrieval.\n",
      "Experiment with models (e.g., `bge-large` vs. `e5-mistral`). - **Chunking\n",
      "Strategy**: Too small → loses context; too large → noisy retrieval. -\n",
      "**Latency**: ANN trade-offs between speed and accuracy. - **Cost**: Managed\n",
      "services (Pinecone) charge per vector; open-source (FAISS) requires\n",
      "infrastructure. - **Freshness**: Vector stores must be updated when source data\n",
      "changes (e.g., via webhooks or cron jobs).  ---  ### **8. Code Example\n",
      "(Python)** ```python from sentence_transformers import SentenceTransformer\n",
      "import faiss import numpy as np  # 1. Load embedding model model =\n",
      "SentenceTransformer(\"all-MiniLM-L6-v2\")  # 2. Create sample data documents = [\n",
      "\"The Earth orbits the Sun once every 365.25 days.\",     \"Photosynthesis is the\n",
      "process by which plants convert sunlight into energy.\",     \"The capital of\n",
      "France is Paris.\" ] embeddings = model.encode(documents)  # 3. Build FAISS index\n",
      "dimension = embeddings.shape[1] index = faiss.IndexFlatL2(dimension)\n",
      "index.add(embeddings)  # 4. Query query = \"What is the process where plants use\n",
      "sunlight?\" query_embedding = model.encode([query]) distances, indices =\n",
      "index.search(query_embedding, k=1)  # 5. Retrieve result print(\"Most relevant\n",
      "document:\", documents[indices[0][0]]) # Output: \"Photosynthesis is the process\n",
      "by which plants convert sunlight into energy.\" ```  ---  ### **9. Advanced\n",
      "Techniques** - **Hybrid Search**: Combine vector search with keyword (BM25) for\n",
      "better precision. - **Reranking**: Use a cross-encoder (e.g., `cross-encoder/ms-\n",
      "marco-MiniLM-L-6-v2`) to reorder retrieved chunks. - **Metadata Filtering**:\n",
      "Retrieve only vectors matching specific criteria (e.g., `date > 2023-01-01`). -\n",
      "**ColBERT**: Token-level retrieval for finer granularity.  ---  ### **10. Future\n",
      "Trends** - **Multimodal RAG**: Stores embeddings for text + images + audio\n",
      "(e.g., using CLIP or ImageBind). - **Real-time Updates**: Vector stores synced\n",
      "with streaming data (e.g., Kafka). - **Personalization**: User-specific vector\n",
      "stores for customized retrieval. - **Federated RAG**: Distributed vector stores\n",
      "for privacy-preserving retrieval.  --- ### **Summary** A **RAG vector store** is\n",
      "the backbone of retrieval-augmented systems, enabling: 1. **Efficient storage**\n",
      "of embeddings. 2. **Fast similarity search** to retrieve relevant context. 3.\n",
      "**Grounded generation** by LLMs, improving accuracy and reducing hallucinations.\n",
      "By combining **vector databases** with **LLMs**, RAG systems bridge the gap\n",
      "between static model knowledge and dynamic, domain-specific data.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call the function and print the result\n",
    "llm_response = call_mistral_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adbee6f-4e90-4d03-8fe1-e70da11978a9",
   "metadata": {},
   "source": [
    "## 3.Advanced RAG\n",
    "### 3.1.Vector search\n",
    "\n",
    "Search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "132f580b-897f-4566-875d-c202c1dd73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_best_match(text_input, records):\n",
    "    best_score = 0\n",
    "    best_record = None\n",
    "    for record in records:\n",
    "        current_score = calculate_cosine_similarity(text_input, record)\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_record = record\n",
    "    return best_score, best_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d842650-1d6d-4fb7-8048-bafa0b362198",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_similarity_score, best_matching_record = find_best_match(query, db_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5857d1f0-b7c3-4577-a4a4-4dfa663a09ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a8127e5-b0fa-4373-9456-13b6389a6728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "652e98d3-846b-464e-9c93-96d00da16e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, best_matching_record)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfa7e004-6036-46e4-814d-50bea90de2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_input=query+\": \"+best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e215d16-1903-4f07-afc7-ff059c54e5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bc5ffc2-2d4e-4356-825b-f9ccf6787a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "The term you've provided is **\"RAG vector store\"**, which is a key component in\n",
      "**Retrieval-Augmented Generation (RAG)** systems—a popular architecture in\n",
      "modern **Natural Language Processing (NLP)** and **Generative AI**. Below is a\n",
      "detailed breakdown of what a **RAG vector store** is, how it works, its purpose,\n",
      "and its role in AI applications.  ---  ### **1. Definition of a RAG Vector\n",
      "Store** A **RAG vector store** is a specialized **database or data structure**\n",
      "designed to store **vectorized data points** (also called **embeddings**). These\n",
      "embeddings are numerical representations of text, images, audio, or other data\n",
      "types, generated by **machine learning models** (e.g., transformers like BERT,\n",
      "sentence-BERT, or CLIP).  In the context of **Retrieval-Augmented Generation\n",
      "(RAG)**, the vector store serves as the **knowledge base** that a generative AI\n",
      "model (like a Large Language Model or LLM) queries to retrieve relevant\n",
      "information before generating a response.  ---  ### **2. Key Components of a RAG\n",
      "Vector Store** #### **A. Vector Embeddings** - Data (e.g., text documents, code\n",
      "snippets, product descriptions) is converted into **dense vector\n",
      "representations** using an **embedding model**. - Example: The sentence *\"The\n",
      "cat sat on the mat.\"* might be converted into a **768-dimensional vector** (if\n",
      "using BERT-base). - These vectors capture **semantic meaning**, so similar\n",
      "content will have similar vectors.  #### **B. Vector Database** - The embeddings\n",
      "are stored in a **vector-optimized database** (e.g., **FAISS, Pinecone,\n",
      "Weaviate, Milvus, Chroma, or Qdrant**). - Unlike traditional databases (which\n",
      "store rows/columns), vector databases are optimized for:   - **Fast similarity\n",
      "search** (finding the closest vectors using metrics like **cosine similarity**\n",
      "or **Euclidean distance**).   - **Scalability** (handling millions/billions of\n",
      "vectors efficiently).   - **Real-time retrieval** (low-latency responses for AI\n",
      "applications).  #### **C. Indexing & Search Mechanism** - The vector store uses\n",
      "**approximate nearest neighbor (ANN) search** to quickly find the most relevant\n",
      "vectors for a given query. - Example: If a user asks, *\"What are the symptoms of\n",
      "diabetes?\"*, the system:   1. Converts the query into a vector.    2. Searches\n",
      "the vector store for the closest matching documents.   3. Retrieves the top-*k*\n",
      "most relevant chunks of text.  ---  ### **3. How a RAG Vector Store Works in a\n",
      "RAG System** RAG combines **retrieval** (from the vector store) and\n",
      "**generation** (from an LLM). Here’s the step-by-step flow:  1. **Data Ingestion\n",
      "& Vectorization**    - Raw data (e.g., PDFs, web pages, internal documents) is\n",
      "**chunked** into smaller pieces.    - Each chunk is passed through an\n",
      "**embedding model** to generate a vector.    - Vectors are stored in the\n",
      "**vector store** with metadata (e.g., source URL, document ID).  2. **Query\n",
      "Processing**    - A user submits a query (e.g., *\"How does photosynthesis\n",
      "work?\"*).    - The query is **vectorized** using the same embedding model.  3.\n",
      "**Retrieval Phase**    - The vector store performs a **similarity search** to\n",
      "find the top-*k* most relevant document chunks.    - Example: If the store\n",
      "contains biology textbooks, it retrieves chunks about photosynthesis.  4.\n",
      "**Augmented Generation**    - The retrieved chunks are **injected into the LLM’s\n",
      "prompt** (e.g., as context).    - The LLM generates a response **grounded in the\n",
      "retrieved data**, reducing hallucinations.  5. **Response Delivery**    - The\n",
      "user receives an answer that combines **retrieved facts** and **generative\n",
      "fluency**.  --- ### **4. Why Use a RAG Vector Store?** | **Benefit**\n",
      "| **Explanation** | |---------------------------|----------------| | **Improved\n",
      "Accuracy**     | Reduces LLM hallucinations by grounding responses in real data.\n",
      "| | **Up-to-Date Knowledge**  | Unlike static LLMs (trained on old data), RAG\n",
      "can fetch **current information** from the vector store. | | **Custom Knowledge\n",
      "Bases**| Enables domain-specific applications (e.g., legal, medical, or\n",
      "enterprise Q&A). | | **Efficiency**            | Avoids retraining the LLM; just\n",
      "update the vector store with new data. | | **Transparency**          | Users can\n",
      "trace answers back to source documents (important for trust and compliance). |\n",
      "---  ### **5. Example Use Cases** 1. **Enterprise Search**    - Replace\n",
      "traditional keyword search with **semantic search** (e.g., finding relevant\n",
      "internal documents). 2. **Chatbots & Virtual Assistants**    - Power customer\n",
      "support bots with **company-specific knowledge** (e.g., FAQs, product manuals).\n",
      "3. **Legal/Medical Q&A**    - Retrieve precise answers from **regulated\n",
      "documents** (e.g., contracts, research papers). 4. **E-Commerce\n",
      "Recommendations**    - Find similar products using **vector similarity** (e.g.,\n",
      "\"Show me shoes like these\"). 5. **Code Assistants**    - Retrieve relevant code\n",
      "snippets from a **vectorized codebase** (e.g., GitHub repositories).  ---  ###\n",
      "**6. Popular Vector Stores for RAG** | **Tool**         | **Type**       | **Key\n",
      "Features** | |------------------|---------------|------------------| | **FAISS**\n",
      "(Meta) | Library       | Fast similarity search, GPU-optimized. | | **Pinecone**\n",
      "| Managed DB    | Serverless, hybrid search (vector + keyword). | | **Weaviate**\n",
      "| Open-source   | Supports graph structures, modular. | | **Milvus**       |\n",
      "Open-source   | Scalable, cloud-native. | | **Chroma**       | Open-source   |\n",
      "Lightweight, easy to deploy. | | **Qdrant**       | Open-source   | Filtering\n",
      "support, high performance. |  ---  ### **7. Challenges & Considerations** -\n",
      "**Data Quality**: Garbage in = garbage out. Poor embeddings lead to poor\n",
      "retrieval. - **Latency**: ANN search must be fast enough for real-time\n",
      "applications. - **Cost**: Storing and searching billions of vectors can be\n",
      "expensive. - **Embedding Model Choice**: The model must align with the use case\n",
      "(e.g., multilingual, domain-specific). - **Hybrid Search**: Combining **vector\n",
      "search** with **keyword search** (e.g., BM25) can improve results.  --- ### **8.\n",
      "Code Example (Python with FAISS)** Here’s a simple example of creating a RAG\n",
      "vector store using **FAISS** and **sentence-transformers**:  ```python from\n",
      "sentence_transformers import SentenceTransformer import faiss import numpy as np\n",
      "# 1. Load an embedding model model = SentenceTransformer('all-MiniLM-L6-v2')  #\n",
      "2. Sample documents documents = [     \"The capital of France is Paris.\",\n",
      "\"Photosynthesis is the process by which plants convert sunlight into energy.\",\n",
      "\"Machine learning is a subset of artificial intelligence.\" ]  # 3. Vectorize the\n",
      "documents embeddings = model.encode(documents)  # 4. Create a FAISS index\n",
      "dimension = embeddings.shape[1] index = faiss.IndexFlatL2(dimension)  # L2\n",
      "distance for similarity index.add(embeddings)  # 5. Query the vector store query\n",
      "= \"What is the process where plants use sunlight?\" query_embedding =\n",
      "model.encode([query])  # 6. Retrieve the most similar document k = 1  # Top 1\n",
      "result distances, indices = index.search(query_embedding, k) print(\"Most\n",
      "relevant document:\", documents[indices[0][0]]) ``` **Output**: ``` Most relevant\n",
      "document: Photosynthesis is the process by which plants convert sunlight into\n",
      "energy. ```  --- ### **9. Future Trends** - **Multimodal RAG**: Storing and\n",
      "retrieving **text + images + audio** in the same vector store. - **Real-Time\n",
      "Updates**: Vector stores that sync with live data (e.g., news, social media). -\n",
      "**Personalization**: User-specific vector stores for tailored responses. -\n",
      "**Federated RAG**: Distributed vector stores for privacy-preserving\n",
      "applications.  --- ### **10. Summary** A **RAG vector store** is the backbone of\n",
      "**Retrieval-Augmented Generation**, enabling AI systems to: 1. **Store**\n",
      "vectorized knowledge (text, images, etc.). 2. **Retrieve** relevant information\n",
      "quickly using similarity search. 3. **Augment** LLM responses with grounded, up-\n",
      "to-date data.  By combining the **strengths of retrieval (precision) and\n",
      "generation (fluency)**, RAG systems powered by vector stores are transforming\n",
      "how we build **knowledge-intensive AI applications**.  Would you like a deeper\n",
      "dive into any specific aspect (e.g., embedding models, vector database\n",
      "benchmarks, or RAG evaluation metrics)?\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call the function and print the result\n",
    "llm_response = call_mistral_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726644b-348c-44ed-8174-e59d2de982c7",
   "metadata": {},
   "source": [
    "### 3.2.Index-based search\n",
    "\n",
    "Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "656e898e-8c8c-43fd-bffa-dddcaf0d378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def setup_vectorizer(records):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(records)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def find_best_match(query, vectorizer, tfidf_matrix):\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
    "    best_score = similarities[0, best_index]\n",
    "    return best_score, best_index\n",
    "\n",
    "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
    "\n",
    "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
    "best_matching_record = db_records[best_index]\n",
    "\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d384c06a-1fff-4e07-aaa6-46a9d14ee043",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "019d20c7-3581-44c5-a86f-62fea1e71470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.407\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2b1bc29-6415-489e-bf36-ed86052f8413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity:, 0.642\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c123b2f-93f2-4cc5-be34-e369c9810edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ability    access  accuracy  accurate     adapt  additional  advancement  \\\n",
      "0   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "1   0.000000  0.000000  0.000000  0.216364  0.000000    0.000000     0.000000   \n",
      "2   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "3   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "4   0.000000  0.000000  0.000000  0.000000  0.000000    0.236479     0.000000   \n",
      "5   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "7   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "8   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "9   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "10  0.186734  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "11  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "12  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "13  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "14  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "15  0.000000  0.172624  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "16  0.000000  0.317970  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "17  0.000000  0.000000  0.000000  0.206861  0.000000    0.000000     0.000000   \n",
      "18  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "19  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "20  0.000000  0.000000  0.000000  0.000000  0.275802    0.000000     0.000000   \n",
      "21  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "22  0.000000  0.174772  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "23  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "24  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "25  0.000000  0.000000  0.228743  0.000000  0.000000    0.000000     0.000000   \n",
      "26  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.173327   \n",
      "27  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
      "\n",
      "      advice  algorithms    allows  ...    vector  vectorized      when  \\\n",
      "0   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "1   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "2   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "3   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "4   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "5   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "6   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "7   0.000000    0.220687  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "8   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "9   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "10  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "11  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.295573   \n",
      "12  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "13  0.000000    0.000000  0.000000  ...  0.200131     0.00000  0.000000   \n",
      "14  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "15  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "16  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "17  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "18  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "19  0.244401    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "20  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "21  0.000000    0.000000  0.291503  ...  0.000000     0.00000  0.000000   \n",
      "22  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "23  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "24  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "25  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "26  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
      "27  0.000000    0.000000  0.000000  ...  0.307719     0.34589  0.000000   \n",
      "\n",
      "       where     which    while     wide      with    within   without  \n",
      "0   0.000000  0.000000  0.00000  0.00000  0.000000  0.260582  0.000000  \n",
      "1   0.000000  0.000000  0.00000  0.00000  0.160278  0.000000  0.000000  \n",
      "2   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "3   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "4   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "5   0.000000  0.247710  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "6   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "7   0.000000  0.179053  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "8   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "9   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "10  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "11  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "12  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "13  0.000000  0.182517  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "14  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "15  0.189283  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "16  0.000000  0.000000  0.00000  0.00000  0.258278  0.000000  0.000000  \n",
      "17  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "18  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "19  0.217430  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "20  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "21  0.000000  0.000000  0.00000  0.00000  0.192110  0.000000  0.291503  \n",
      "22  0.000000  0.000000  0.00000  0.21541  0.141963  0.000000  0.000000  \n",
      "23  0.000000  0.000000  0.32932  0.00000  0.217033  0.000000  0.000000  \n",
      "24  0.000000  0.000000  0.00000  0.00000  0.134513  0.000000  0.000000  \n",
      "25  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "26  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "27  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[28 rows x 297 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def setup_vectorizer(records):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(records)\n",
    "\n",
    "    # Convert the TF-IDF matrix to a DataFrame for display purposes\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(tfidf_df)\n",
    "\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "vectorizer, tfidf_matrix = setup_vectorizer(db_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8140db5d-8efd-479d-888d-4ba0f29f276c",
   "metadata": {},
   "source": [
    "### Augmented input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e45f850c-b277-4058-9e72-f281cfa4a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_input=query+\": \"+best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3d0a15a-6318-4a9d-9ff3-89d4e80345e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc067a87-033d-4dab-a40b-10fa9e176fc6",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50dd4270-9ace-4e2f-82da-ed42424719c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "The term you’ve provided is **\"RAG vector store\"** (often referred to in the\n",
      "context of **Retrieval-Augmented Generation (RAG)**). Below is a detailed\n",
      "explanation of what it means, its purpose, components, and how it functions in\n",
      "Natural Language Processing (NLP) and AI systems.  ---  ### **1. Definition of a\n",
      "RAG Vector Store** A **RAG vector store** is a specialized **database or\n",
      "dataset** designed to store **vectorized data points** (also called\n",
      "**embeddings**). These embeddings are numerical representations of data (e.g.,\n",
      "text, images, or other modalities) generated by machine learning models (like\n",
      "neural networks). The vector store enables efficient **storage, retrieval, and\n",
      "similarity search** of these embeddings, which is critical for RAG systems.  ---\n",
      "### **2. Key Components of a RAG Vector Store** #### **a) Vector Embeddings** -\n",
      "Data (e.g., text documents, sentences, or paragraphs) is converted into **dense\n",
      "vectors** (arrays of floating-point numbers) using **embedding models** (e.g.,\n",
      "BERT, Sentence-BERT, or OpenAI’s `text-embedding-ada-002`). - Example: The\n",
      "sentence *\"The cat sat on the mat\"* might be converted into a 768-dimensional\n",
      "vector like `[0.23, -0.45, 0.12, ..., 0.78]`.  #### **b) Storage Backend** - The\n",
      "vector store uses optimized databases or libraries to store and index these\n",
      "embeddings. Popular tools include:   - **FAISS** (Facebook AI Similarity Search)\n",
      "- **Pinecone**   - **Weaviate**   - **Milvus**   - **Chroma**   - **PostgreSQL\n",
      "with pgvector** (for SQL-based vector storage)   - **Redis with RediSearch** -\n",
      "These tools support **approximate nearest neighbor (ANN) search**, which is\n",
      "faster than brute-force search for large datasets.  #### **c) Indexing\n",
      "Mechanism** - The vector store organizes embeddings into an **index** to enable\n",
      "fast retrieval. Techniques include:   - **Locality-Sensitive Hashing (LSH)**   -\n",
      "**Hierarchical Navigable Small World (HNSW)**   - **Inverted File Index (IVF)**\n",
      "- **Product Quantization (PQ)**  #### **d) Retrieval Interface** - The store\n",
      "provides APIs or query methods to:   - **Search for similar vectors** (e.g.,\n",
      "\"Find the top 5 vectors closest to this query vector\").   - **Add/update/delete\n",
      "embeddings** dynamically.   - **Filter results** using metadata (e.g., \"Only\n",
      "retrieve documents from 2023\").  ---  ### **3. Role in Retrieval-Augmented\n",
      "Generation (RAG)** RAG is a framework that combines **retrieval-based methods**\n",
      "(fetching relevant information from a dataset) with **generative AI** (e.g.,\n",
      "large language models like LLMs). The vector store is the \"retrieval\" part of\n",
      "RAG. Here’s how it works:  #### **Step 1: Preprocessing (Vectorization)** - A\n",
      "corpus of documents (e.g., Wikipedia articles, internal company docs) is split\n",
      "into chunks (e.g., sentences or paragraphs). - Each chunk is converted into a\n",
      "vector embedding using an embedding model. - These embeddings are stored in the\n",
      "**vector store**.  #### **Step 2: Query Processing** - When a user asks a\n",
      "question (e.g., *\"What is the capital of France?\"*), the question is also\n",
      "converted into a vector embedding. - The vector store **retrieves the top-k most\n",
      "similar embeddings** (e.g., chunks about France’s capital) using similarity\n",
      "metrics like **cosine similarity** or **Euclidean distance**.  #### **Step 3:\n",
      "Augmented Generation** - The retrieved chunks (now in text form) are passed to a\n",
      "**generative model** (e.g., GPT-4) along with the original query. - The model\n",
      "uses this **context** to generate a more accurate, grounded response (e.g.,\n",
      "*\"The capital of France is Paris\"*).  --- ### **4. Why Use a Vector Store in\n",
      "RAG?** | **Benefit**               | **Explanation**\n",
      "| |---------------------------|-------------------------------------------------\n",
      "--------------------------------| | **Efficiency**            | ANN search is\n",
      "much faster than brute-force comparison for large datasets.       | |\n",
      "**Scalability**           | Can handle millions/billions of embeddings (e.g.,\n",
      "Pinecone scales to 10M+ vectors). | | **Dynamic Updates**       | New data can\n",
      "be added without retraining the entire model.                     | | **Semantic\n",
      "Search**       | Retrieves data based on **meaning** (semantics) rather than\n",
      "keyword matching.   | | **Grounding LLM Responses** | Reduces hallucinations by\n",
      "providing factual context from the vector store.     |  ---  ### **5. Example\n",
      "Workflow** #### **Use Case: Customer Support Chatbot** 1. **Data Preparation**:\n",
      "- Store all FAQs, product manuals, and support tickets as embeddings in a vector\n",
      "store (e.g., Weaviate). 2. **User Query**:    - User asks: *\"How do I reset my\n",
      "password?\"*    - The query is embedded into a vector. 3. **Retrieval**:    - The\n",
      "vector store returns the top 3 most similar FAQ chunks (e.g., *\"Forgot password?\n",
      "Click 'Reset' on the login page.\"*). 4. **Generation**:    - The LLM combines\n",
      "the retrieved chunks with the query to generate a step-by-step answer.  ---  ###\n",
      "**6. Challenges and Considerations** | **Challenge**               |\n",
      "**Solution**                                                                 | |\n",
      "-----------------------------|--------------------------------------------------\n",
      "----------------------------| | **High Dimensionality**     | Use dimensionality\n",
      "reduction (e.g., PCA) or optimized indexing (HNSW).     | | **Data Freshness**\n",
      "| Implement real-time updates or periodic re-indexing.                        |\n",
      "| **Cost**                    | Open-source tools (FAISS, Milvus) reduce costs\n",
      "vs. managed services (Pinecone). | | **Bias in Retrieval**       | Diversify\n",
      "training data or use re-ranking models (e.g., Cross-Encoders).     | |\n",
      "**Latency**                 | Trade off between accuracy (exhaustive search) and\n",
      "speed (ANN).              |  ---  ### **7. Tools and Libraries for Vector\n",
      "Stores** | **Tool**          | **Type**       | **Key Features**\n",
      "| |-------------------|----------------|----------------------------------------\n",
      "-----------------------------------------| | **FAISS**         | Library\n",
      "| GPU-optimized, supports L2/cosine similarity, developed by Meta.\n",
      "| | **Pinecone**      | Managed Service | Serverless, hybrid search (vector +\n",
      "keyword), easy integration.               | | **Weaviate**      | Open-Source\n",
      "| Graph-based, supports cross-references, modular (e.g., add custom models).\n",
      "| | **Milvus**        | Open-Source    | Cloud-native, scalable, supports\n",
      "distributed deployments.                     | | **Chroma**        | Open-Source\n",
      "| Lightweight, designed for LLM applications, easy to self-host.\n",
      "| | **pgvector**      | PostgreSQL Ext | Adds vector search to PostgreSQL, good\n",
      "for hybrid SQL/vector workflows.        |  ---  ### **8. Code Example (Python\n",
      "with FAISS)** ```python import faiss import numpy as np from\n",
      "sentence_transformers import SentenceTransformer  # 1. Load an embedding model\n",
      "model = SentenceTransformer('all-MiniLM-L6-v2')  # 2. Create sample data\n",
      "documents = [     \"Paris is the capital of France.\",     \"Berlin is the capital\n",
      "of Germany.\",     \"Tokyo is the capital of Japan.\" ] embeddings =\n",
      "model.encode(documents)  # Shape: (3, 384)  # 3. Build a FAISS index dimension =\n",
      "embeddings.shape[1] index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
      "index.add(embeddings)  # 4. Search for a query query = \"What is the capital of\n",
      "Germany?\" query_embedding = model.encode([query]) k = 1  # Top 1 result\n",
      "distances, indices = index.search(query_embedding, k)  print(\"Most similar\n",
      "document:\", documents[indices[0][0]]) # Output: \"Berlin is the capital of\n",
      "Germany.\" ```  ---  ### **9. Advanced Concepts** - **Hybrid Search**: Combine\n",
      "vector search with keyword search (e.g., using BM25 + embeddings). - **Re-\n",
      "ranking**: Use a cross-encoder to re-rank retrieved results for higher accuracy.\n",
      "- **Metadata Filtering**: Filter vectors by attributes (e.g., \"only retrieve\n",
      "documents from 2023\"). - **Quantization**: Reduce vector precision (e.g.,\n",
      "float32 → int8) to save memory with minimal accuracy loss.  ---  ### **10. Real-\n",
      "World Applications** 1. **Search Engines**: Semantic search (e.g., Google’s\n",
      "BERT-based results). 2. **Recommendation Systems**: \"Users who liked X also\n",
      "liked Y\" (based on item embeddings). 3. **Legal/Medical QA**: Retrieve relevant\n",
      "case laws or research papers for a query. 4. **Code Search**: Find similar code\n",
      "snippets in a repository (e.g., GitHub Copilot). 5. **E-commerce**: Product\n",
      "search by image (vectorize images and find visually similar items).  ---  ###\n",
      "**11. Future Trends** - **Multimodal Vector Stores**: Store and retrieve\n",
      "embeddings for text, images, audio, and video (e.g., CLIP for image-text). -\n",
      "**Federated Vector Stores**: Distributed stores for privacy-preserving\n",
      "retrieval. - **Self-Supervised Learning**: Improve embedding models with less\n",
      "labeled data. - **Edge Deployment**: Lightweight vector stores for mobile/IoT\n",
      "devices.  ---  ### **Summary** A **RAG vector store** is the backbone of\n",
      "retrieval-augmented systems, enabling efficient storage and search of vectorized\n",
      "data. It bridges the gap between static knowledge (stored in embeddings) and\n",
      "dynamic generation (via LLMs), making AI systems more accurate, interpretable,\n",
      "and grounded in real-world data. By leveraging approximate nearest neighbor\n",
      "search and optimized indexing, these stores handle large-scale data while\n",
      "maintaining low latency, making them indispensable for modern NLP applications.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_mistral_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b51806-4978-4ec2-9f81-16a6e0325441",
   "metadata": {},
   "source": [
    "## 4.Modular RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "009ef2b4-d4ea-453e-b72f-faaa7000ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class RetrievalComponent:\n",
    "    def __init__(self, method='vector'):\n",
    "        self.method = method\n",
    "        if self.method == 'vector' or self.method == 'indexed':\n",
    "            self.vectorizer = TfidfVectorizer()\n",
    "            self.tfidf_matrix = None\n",
    "\n",
    "    def fit(self, records):\n",
    "      self.documents = records  # Initialize self.documents here\n",
    "      if self.method == 'vector' or self.method == 'indexed':\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        if self.method == 'keyword':\n",
    "            return self.keyword_search(query)\n",
    "        elif self.method == 'vector':\n",
    "            return self.vector_search(query)\n",
    "        elif self.method == 'indexed':\n",
    "            return self.indexed_search(query)\n",
    "\n",
    "    def keyword_search(self, query):\n",
    "        best_score = 0\n",
    "        best_record = None\n",
    "        query_keywords = set(query.lower().split())\n",
    "        for index, doc in enumerate(self.documents):\n",
    "            doc_keywords = set(doc.lower().split())\n",
    "            common_keywords = query_keywords.intersection(doc_keywords)\n",
    "            score = len(common_keywords)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_record = self.documents[index]\n",
    "        return best_record\n",
    "\n",
    "    def vector_search(self, query):\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
    "        best_index = similarities.argmax()\n",
    "        return db_records[best_index]\n",
    "\n",
    "    def indexed_search(self, query):\n",
    "        # Assuming the tfidf_matrix is precomputed and stored\n",
    "        query_tfidf = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
    "        best_index = similarities.argmax()\n",
    "        return db_records[best_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4baf39-e9c1-4393-9521-e377c2c6ded0",
   "metadata": {},
   "source": [
    "### Modular RAG Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "165937c3-f7c0-433d-a1f4-204276851dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'\n",
    "retrieval.fit(db_records)\n",
    "best_matching_record = retrieval.retrieve(query)\n",
    "\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841f220-0530-4e83-a71b-a37c8df0a3c4",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74de8f4d-5465-4462-8023-9575bbc32049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cosine Similarity Score: 0.407\n",
      "Response:\n",
      "---------------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a51aa13-96f4-4721-8d0a-90f87084a241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
      "Enhanced Similarity: 0.641582812483307\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Similarity\n",
    "response = best_matching_record\n",
    "print(query,\": \", response)\n",
    "similarity_score = calculate_enhanced_similarity(query, response)\n",
    "print(\"Enhanced Similarity:\", similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f836d4-4f93-419a-829b-3d2a80f20a8c",
   "metadata": {},
   "source": [
    "#### Augmented Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c72687c-0344-4051-8a32-f816a6808405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------------\n",
      "define a rag store A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_input=query+ \" \"+ best_matching_record\n",
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1374def4-51cd-48e7-8fd6-43a301ed61ea",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c07ba70a-b11a-4258-b38d-235e1eac8266",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'call_llm_with_full_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call the function and print the result\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m llm_response \u001b[38;5;241m=\u001b[39m call_llm_with_full_text(augmented_input)\n\u001b[0;32m      3\u001b[0m print_formatted_response(llm_response)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'call_llm_with_full_text' is not defined"
     ]
    }
   ],
   "source": [
    "# Call the function and print the result\n",
    "llm_response = call_mistral_with_full_text(augmented_input)\n",
    "print_formatted_response(llm_response)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f2726-27a0-4d9f-b788-c201654e95bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
